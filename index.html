<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta property="og:title" content="VSG-HOI: Enhancing Understanding of Human-Object Interaction in Videos with Visual, Structural and Global Information"/>
  <meta property="og:url" content="https://cyber-bobo.github.io/VSG-HOI/"/>
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <title>VSG-HOI: Enhancing Understanding of Human-Object Interaction in Videos with Visual, Structural and Global Information</title>
  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link rel="icon" href="static/figures/icon2.png">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="static/js/audioPlayer.js" defer></script>
</head>

<body>

  
<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://cyber-bobo.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
<!--           <a class="navbar-item" href="https://evm7.github.io/Self-AWare/">
            Self-AWare
          </a>
          <a class="navbar-item" href="https://evm7.github.io/I-CTRL/">
            I-CTRL
          </a>
          <a class="navbar-item" href="https://evm7.github.io/ECHO/">
            ECHO
          </a>
          <a class="navbar-item" href="https://evm7.github.io/HOI4ABOT_page/">
            HOI4ABOT
          </a>
           <a class="navbar-item" href="https://evm7.github.io/UNIMASKM-page/">
            UNIMASK-M
          </a>
          <a class="navbar-item" href="https://evm7.github.io/icvae-page/">
            IntentionCVAE
          </a>
          <a class="navbar-item" href="https://evm7.github.io/HOIGaze-page/">
            HOIGaze
          </a>
            <a class="navbar-item" href="https://evm7.github.io/2CHTR-page/">
            2CHTR
          </a> -->
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="publication-header">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <!-- <div class="columns is-centered"> -->
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">VSG-HOI: Enhancing Understanding of Human-Object Interaction in Videos with Visual, Structural and Global Information</h1>
          <div class="is-size-3 publication-authors">
            TCSVT, 2024
          </div>
        </div>
    </div>
  </div>

</section>

<section class="publication-author-block">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <div class="is-size-5 publication-authors">
		  <span class="author-block">Bohong Wu,</span>
		  <span class="author-block">Qing Gao*,</span>
		  <span class="author-block">Zhaojie Ju,</span>
<!--             <span class="author-block"><a href="https://www.tuwien.at/en/etit/ict/asl/team/yashuai-yan" target="_blank">Yashuai Yan*</a>,</span>
           <span class="author-block"><a href="https://evm7.github.io/" target="_blank">Esteve Valls Mascaro*</a>,</span>
            <span class="author-block"><a href="https://www.tuwien.at/etit/ict/asl/team/dongheui-lee" target="_blank">Dongheui Lee</a> -->
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Sun Yat-Sen University (Shenzhen)</span> 
            <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Corresponding Author</small></span>
          </div>
          


          <div class="column has-text-centered">
            <div class="publication-links">
<!--               <span class="link-block">
                <a href="https://arxiv.org/abs/2309.05310" target="_blank"
                  class="external-link button is-normal is-rounded">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->

              
              <!-- PDF Link. -->
<!--               <span class="link-block">
               <a href="https://ieeexplore.ieee.org/document/10375150" target="_blank"
                class="external-link button is-normal is-rounded">
                 <span class="icon">
                   <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->

		<!-- Colab Link.  -->
              <span class="link-block">
                <a href="https://github.com/Cyber-BoBo/VSG-HOI" target="_blank"
                class="external-link button is-normal is-rounded">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
             </span>

<!--              <span class="link-block">-->
<!--                <a href="ADD HERE REPLICATE IF NEEDED" target="_blank"-->
<!--                class="external-link button is-normal is-rounded">-->
<!--                <span class="icon">-->
<!--                  <i class="fas fa-rocket"></i>-->
<!--                </span>-->
<!--                <span>Demo</span>-->
<!--              </a>-->
<!--              </span>-->
              <!-- </span> -->
              <!-- Colab Link. -->
		    <!-- Play/Pause button and audio -->
<!-- 		    <span class="link-block">
		      <span class="audio-player" style="display: inline-block; vertical-align: middle;">
			<button id="audio-control-button" onclick="toggleAudio()" class="button is-normal is-rounded">
			  <span class="icon">
			    <i id="play-icon" class="fas fa-play"></i>
			  </span>
			  <span id="play-text">Podcast</span>
			</button>
			<audio id="audio-file" src="static/audio/PodCastAudio.wav" type="audio/wav" style="display: none;"></audio>
		  </span>
	    </span> -->
	  </div>

		 <!-- Hidden Playback Controls + Disclaimer -->
	  <div id="audio-controls" style="display: none; margin-top: 15px;">
	    <div>
	      <input id="seek-bar" type="range" min="0" value="0" step="1" style="width: 100%;">
	      <span id="time-remaining">0:00</span> / <span id="duration">0:00</span>
	    </div>
	    
	    <!-- Disclaimer -->
	    <div class="disclaimer" style="margin-top: 10px;">
  <p><strong>Disclaimer:</strong> This audio was generated by AI <a href="https://notebooklm.google/" target="_blank">NotebookLM</a> and might contain false or misleading information about the paper. Please refer to the original paper for accurate details.</p>

	    </div>
	  </div>

	  <!-- Toggle Visibility Icon -->
	  <div id="toggle-visibility-icon" style="display: none; text-align: right; margin-top: 5px;">
	    <i id="visibility-icon" class="fas fa-eye" style="cursor: pointer;" onclick="toggleControls()"></i>
	  </div>
	</div>

        </div>
      </div>
    </div>
  </div>
</section>

<!-- 
<section class="hero is-small">
  <!~~ <div class="hero-body"> ~~>
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!~~ <div id="results-carousel" class="carousel results-carousel"> ~~>
      <div class="container">
      <div class="item">
      <div class="column is-centered has-text-centered">
        <img src="static/figures/teaser.png" alt="UNIMASKM"/>
      </div>

    </div>
  </div>
 <!~~  </div> ~~>
  </div>
  </div>
 <!~~  </div> ~~>
</section>
 -->

<!--   <section class="hero is-small">
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="container">
        <div class="item">
          <p style="margin-bottom: 30px">
 
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <source src="static/figures/videos/overview.mp4"
          type="video/mp4">
        </video>
        </p>
        </div>
    </div>
  </div>
  </div>
  </div>
</section> -->
    
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">

        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
		Human-Object Interaction (HOI) exists in every aspect of our lives.
		HOI recognition is crucial for computers to learn and understand human actions.
		However, HOIs in videos involve temporal dynamics, which becomes more complex for long-term actions and subjects with multiple interaction.
		This making it challenging to accurately recognize the interactions.
		To enhance the understanding of HOIs, this paper proposes a visual, structural and global information fusion network for HOI recognition (VSG-HOI), which improves HOI recognition accuracy by extracting three different types of HOI information from RGB images, structural points and center points of each entity. 
		VSG-HOI has three different branches that process visual, structural, and global information separately. For the visual branch, this paper utilizes an object detector to perform a rough feature extraction on RGB image of humans and objects.
		For structural and global branches, this paper has designed two different information enhancement modules.
		The structural information enhancement module can learn the connection relationships and temporal dependencies in the structural graph.
		The global information enhancement module learns the potential relationship between the global relative position of humans and objects and their interaction types.
		Finally, an attention-based multimodal fusion strategy is designed to achieve effective feature fusion.
		We have validated the effectiveness of VSG-HOI on MPHOI-72, CAD-120 and Bimanual Actions datasets.
		Experimental results show the high performance of our proposed VSG-HOI compared to state-of-the-art methods.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">

      <div class="column is-centered has-text-centered">
        <img src="static/figures/overview.jpg" alt="Motivation of our model"/>
      </div>    
  </div>
</div>
</div>
</section> -->

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
         <h2 class="title is-3">Architecture</h2> 
        <div class="content has-text-justified">
          <p>
		  VSG-HOI takes inputs through pre-trained Faster R-CNN and pose estimation (marked with snowflake).
		  The feature extraction part includes three branches: visual, structural, and global.
		  For the structural branch and global branch, structural enhancement module (SEM) and global enhancement module (GEM) is designed to enhance feature expression.
		  Features from three branches are fused by an attention-based multimodal fusion strategy.
          </p>
        <div class="columns is-centered has-text-centered">
            <div class="item">
          <p style="margin-bottom: 30px">
<!--
        <video poster="" id="tree" autoplay controls muted loop height="100%">
          <source src="static/figures/MDM-page.mp4"
          type="video/mp4">
        </video>
 --><br><br>
        </p>
        </div>
            </div>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="column is-centered has-text-centered">
        <img src="static/figures/VSG-HOI-Structure.jpg" alt="Architecture of our ImitationNet"/>
      </div>    
  </div>
</div>
</div>
</section>

    
<!--   <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          
      <div class="column is-centered has-text-centered">
        <p><b> “A human and TiaGo are moving among obstacles”</b></p>
                <video poster="" id="tree" autoplay controls muted loop height="100%">
                  <source src="static/figures/videos/s0_moving.mp4" type="video/mp4">
                </video>
      </div>
      <div class="column is-centered has-text-centered">
        <p><b> “A human and TiaGo are moving among obstacles”</b></p>
                <video poster="" id="tree" autoplay controls muted loop height="100%">
                  <source src="static/figures/videos/s0_moving2.mp4" type="video/mp4">
                </video>
      </div>
      <div class="column is-centered has-text-centered">
        <p><b> “A human and TiaGo are moving among obstacles”</b></p>
                <video poster="" id="tree" autoplay controls muted loop height="100%">
                  <source src="static/figures/videos/s2_moving.mp4" type="video/mp4">
                </video>
      </div>
        <div class="column is-centered has-text-centered">
        <p><b> “A human is performing random movements, which TiaGo imitates.”</b></p>
                <video poster="" id="tree" autoplay controls muted loop height="100%">
                  <source src="static/figures/videos/random.mp4" type="video/mp4">
                </video>
      </div>
          </div>
      </div>
    </div>
  </section> -->


    
<!--   <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          
      <div class="column is-centered has-text-centered">
        <p><b> “A man touches his head with his right arm”</b></p>
                <video poster="" id="tree" autoplay controls muted loop height="100%">
                  <source src="static/figures/videos/text2motion/A man touches his head with his right arm (x4).mp4" type="video/mp4">
                </video>
      </div>
      <div class="column is-centered has-text-centered">
        <p><b> “A person is dancing by moving the arms”</b></p>
                <video poster="" id="tree" autoplay controls muted loop height="100%">
                  <source src="static/figures/videos/text2motion/A person is dancing by moving the arms (x4).mp4" type="video/mp4">
                </video>
      </div>
      <div class="column is-centered has-text-centered">
        <p><b> “A person is performing a handshake”</b></p>
                <video poster="" id="tree" autoplay controls muted loop height="100%">
                  <source src="static/figures/videos/text2motion/A person is performing a hanshake (x4).mp4" type="video/mp4">
                </video>
      </div>
      <div class="column is-centered has-text-centered">
        <p><b> “A person waves with his two hands.”</b></p>
                <video poster="" id="tree" autoplay controls muted loop height="100%">
                  <source src="static/figures/videos/text2motion/A person waves with his two hands (x4).mp4" type="video/mp4">
                </video>
      </div>
          </div>
      </div>
    </div>
  </section> -->

<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Human Motion Retargeting From Key Poses</h2>
          <p>
      Thanks to our training contrastive learning approach, ImitationNet builts a tractable latent space where similar poses are pushed together while dissimilar are pushed apart. Our model is then able to generate movements between key poses by uniquely interpolating the representation space of two given poses in the latent space.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>  -->

    
<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Comparison with the baseline</h2>
          <p>
        We compare our ImitationNet (TiaGo robot on the left of the simulation) with the previous baseline model (located on the right). Our results shows the smooth and accuracy of the pose retargeting, which outperforms previous state-of-the-art.         </p>
        </div>
      </div>
    </div>
  </div>
</section>  -->

<!-- 
      <section class="hero is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          
      <div class="column is-centered has-text-centered">
              <video poster="" id="tree" autoplay controls muted loop height="100%">
                <source src="static/figures/videos/baselines/baseline1.mp4" type="video/mp4">
              </video>
      </div>
      <div class="column is-centered has-text-centered">
              <video poster="" id="tree" autoplay controls muted loop height="100%">
                <source src="static/figures/videos/baselines/baseline2.mp4" type="video/mp4">
              </video>
      </div>
        </div>
      </div>
    </div>
  </section> -->
    
<!--     
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
@INPROCEEDINGS{10375150,
  author={Yan, Yashuai and Mascaro, Esteve Valls and Lee, Dongheui},
  booktitle={2023 IEEE-RAS 22nd International Conference on Humanoid Robots (Humanoids)}, 
  title={ImitationNet: Unsupervised Human-to-Robot Motion Retargeting via Shared Latent Space}, 
  year={2023},
  volume={},
  number={},
  pages={1-8},
  keywords={Robot motion;Measurement;Interpolation;Three-dimensional displays;Humanoid robots;Aerospace electronics;Skeleton},
  doi={10.1109/Humanoids57100.2023.10375150}}
</code></pre>
    </div>
</section> -->




<footer class="footer">
 <!--  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
      href="https://homes.cs.washington.edu/~kpar/nerfies/videos/nerfies_paper.pdf">
      <i class="fas fa-file-pdf"></i>
    </a>
    <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
      <i class="fab fa-github"></i>
    </a>
  </div> -->
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This website is licensed under a <a rel="license"
          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
      <p>
        Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. If you want to reuse their <a
        href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
      </p>
    </div>
  </div>
</div>
</div>
</footer>

  </body>
  </html>
